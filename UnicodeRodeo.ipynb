{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python390jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.9.0 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# UNICODE RODEO\n",
    "## A Short Introductory Adventure with Unicode\n",
    "Designed for those who may be working with multiple alphabets or character sets in python. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## The Basics of Characters and Bytes\n",
    "\n",
    "Character information is stored in bytes. Each character takes up one or two bytes, depending on how its stored. There are a number of different systems for storing information about text. \n",
    "\n",
    "You may be familiar with ASCII art: https://www.asciiart.eu/\n",
    "\n",
    "*ASCII* stands for American Standard Code for Information Interchange, but that's not super important. It's an old school way of encoding character information from the days of telegraphs. It only deals with 128 characters, which is not enough! Now, Unicode is dominant since it's a much more versatile system of encoding text. \n",
    "\n",
    "When saving character information, it's at some point encoded, or turned into a format that's more useful for the computer. Python gives us a number of useful functions for this. Let's try out the encode() function. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "text1 = \"oboe\"\n",
    "print(text1, len(text1)) # prints the string and it's length in bytes\n",
    "enc_1 = text1.encode('ascii') # encodes the string in ascii\n",
    "print(enc_1, len(enc_1)) # prints the string and it's length in bytes"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 1,
   "outputs": []
  },
  {
   "source": [
    "Great. It gives us the byte information. But it's not very interesting. Let's try something better. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_text = \"türkçe\"\n",
    "print(tr_text, len(tr_text))\n",
    "enc_tr = tr_text.encode('utf-8') # encodes the string in utf-8\n",
    "print(enc_tr, len(enc_tr)) \n"
   ]
  },
  {
   "source": [
    "Well that's a problem. It looks like ASCII wasn't up to the task of encoding all these characters, and generated a Unicode Encode Error. Luckily, we can fix this. \n",
    "\n",
    "## Unicode\n",
    "\n",
    "Unicode is a widely adoped standard for encoding lots of characters on the web.\n",
    "And Unicode's website here: https://home.unicode.org/\n",
    "Unicode consists of a 'code point' for each character, a unique idenitifying number that can be used to reference it and go get more infromation about characters. For example: \"U+0041\" is the code point for an uppercase \"A\". Another example is 'U+1f647', which is an emoji. Characters of the same alphabet typically fall into a similar unicode code point range. Check out the database here: https://www.unicode.org/ucd/\n",
    "\n",
    "### UTF-8\n",
    "\n",
    "UTF-8 is the most common character encoding. It uses 1 byte for character for the 128 characters defined by ASCII commonly used in english language text, and between 2 and 4 for all others. That makes it fastest for a lot of latin based languages. \n",
    "Let's try that for our turkish text. \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_text = \"türkçe\"\n",
    "print(tr_text, len(tr_text), \"characters long unencoded\") \n",
    "enc_tr = tr_text.encode('utf-8') \n",
    "print(enc_tr, len(enc_tr), \"bytes long in utf-8 encoding\") "
   ]
  },
  {
   "source": [
    "Great. It's encoded our problem characters, and only added 2 bytes to our total byte useage.\n",
    "Look at how the encoding takes place at the special characters. \n",
    "\n",
    "You might notice that this doesn't look like a Unicode code point - that's because it's an encoded version. If you want to find out the code point, you can use the ord() function to get a code point, and the chr() function to turn it back to a character. \n",
    "Let's look at 'A' U+0041."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_point = ord('A')\n",
    "print(code_point, \"is the code point for 'ü', but it's in Python's base 10\")\n",
    "character = chr(code_point)\n",
    "print(character, f\"is the character for code point {code_point}\")\n",
    "hex_code_point= hex(code_point)\n",
    "print(hex_code_point, \"is the same number in hexadecimal! It's a match!\")"
   ]
  },
  {
   "source": [
    "But what about other Unicode encodings? \n",
    "\n",
    "### UTF-16\n",
    "\n",
    "UTF-16 uses 2 bytes for most characters, with 4 used for some others.  If you're interested in more details on the finer points of unicode encoding, check out: https://medium.com/swlh/what-is-utf-16-63755027eb29\n",
    "\n",
    "\n",
    "### UTF-32 \n",
    "\n",
    "UTF-32 uses 4 bytes for all characters, and is not widely used because of how much space it takes up. \n",
    "\n",
    "Let's try these out on some text."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ar = \"الصقر\"\n",
    "print(text_ar, len(text_ar), '5 character arabic text')\n",
    "enc_ar = text_ar.encode('utf8') # note that it still works without the hyphen\n",
    "print(enc_ar, len(enc_ar), 'bytes long in utf-8')\n",
    "\n",
    "enc_ar_2 = text_ar.encode('utf-16')\n",
    "# contains 10 bytes for 5 chars\n",
    "print(enc_ar_2, len(enc_ar_2), 'bytes long in utf-16')\n",
    "print(\"\\n\")"
   ]
  },
  {
   "source": [
    "As you can see, UTF-8 seems to be the better choice for encoding Arabic and similar alphabets. \n",
    "What about Korean?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kr_text = \"안녕하세요\"\n",
    "print(kr_text , len(kr_text), '5 character Korean text')\n",
    "enc_kr = kr_text.encode('utf-8')\n",
    "print(enc_kr, len(enc_kr), 'bytes long in utf-8')\n",
    "print(\"\\n\")\n",
    "\n",
    "enc_kr_2 = kr_text.encode('utf-16')\n",
    "print(enc_kr_2, len(enc_kr_2), 'bytes long in utf-16')\n",
    "print(\"\\n\")\n",
    "\n",
    "enc_kr_3 = kr_text.encode('utf-32')\n",
    "print(enc_kr_3, len(enc_kr_3), 'bytes long in utf-32\\n')\n"
   ]
  },
  {
   "source": [
    "As you can see, UTF-16 would be your best choice for encoding a significant amount of Korean Text.\n",
    "\n",
    "\n",
    "Thank you for coming to my Unicode Rodeo! \n",
    "\n",
    "If you want to see an appliction of Unicode code points, I made a project a while ago that looks at the first letter of a string to determine the code point. If the code point falls into the arabic range, it applies a right-to-left filter. \n",
    "You can see it here: https://github.com/AeronRoemer/Employee-Directory-Display"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}